{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keyword_extraction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMbJr9EQgFRI"
      },
      "outputs": [],
      "source": [
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOURCES--\n",
        "  1. https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
        "  2. https://maartengr.github.io/KeyBERT/\n",
        "  3. https://github.com/ibatra/BERT-Keyword-Extractor\n",
        "\n",
        "\n",
        "for sentence transformers\n",
        "  1. https://www.sbert.net/docs/package_reference/SentenceTransformer.html"
      ],
      "metadata": {
        "id": "UblBd-ZAle3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "8KxX8cwCwuBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "class Keyword_Extractor:\n",
        "    def __init__(self,model):\n",
        "        if isinstance(model,str):\n",
        "            self.model = SentenceTransformer(model)\n",
        "        else:\n",
        "            self.model = model\n",
        "            \n",
        "    def get_ngrams(self,text, n_range=(1,1)):\n",
        "        n_grams = []\n",
        "        a,b = n_range\n",
        "        for i in range(a,b+1):\n",
        "            __ = ngrams(word_tokenize(text), i)\n",
        "            _ = [' '.join(grams) for grams in __]\n",
        "            n_grams = n_grams + _\n",
        "        return n_grams\n",
        "\n",
        "    def clean_string(self,string):\n",
        "        # string = string.encode(\"utf-8\")\n",
        "        # string = BeautifulSoup(string,'lxml').text\n",
        "        string = string.lower()\n",
        "        #special_characters = [\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"+\",\"=\",\"?\",\"'\",\"{\",\"}\",\"[\",\"]\",\"<\",\">\",\"~\",\"`\",\":\",\";\",\"|\",'\\n']\n",
        "        special_characters = [\"!\",\"$\",\"%\",\"^\",\"&\",\"*\",\"+\",\"=\",\"?\",\"'\",\"{\",\"}\",\"[\",\"]\",\"<\",\">\",\"~\",\"`\",\":\",\";\",\"|\",'\\n']\n",
        "        string = re.sub('http[s]?://\\S+', '', string)\n",
        "        string=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",string)\n",
        "        for a in special_characters:\n",
        "            string = string.replace(a,'')\n",
        "        return string\n",
        "    \n",
        "    def find_simillarity_score(self,sentence1,sentence2):\n",
        "        # encode sentences to get their embeddings\n",
        "        embedding1 = self.model.encode(sentence1, convert_to_tensor=True)\n",
        "        embedding2 = self.model.encode(sentence2, convert_to_tensor=True)\n",
        "        # compute similarity scores of two embeddings\n",
        "        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "        return cosine_scores\n",
        "    \n",
        "    def extract_keywords(self,sentence,ngram_range=(1,2),top_k=5,\n",
        "                         do_lower=True,do_cleaning=False):\n",
        "        if do_lower:\n",
        "            sentence = sentence.lower()\n",
        "        if do_cleaning:\n",
        "            sentence = self.clean_string(sentence)\n",
        "        n_grams = self.get_ngrams(sentence, n_range=ngram_range)\n",
        "        n_grams = list(set([a.strip() for a in n_grams]))\n",
        "        # encode corpus to get corpus embeddings\n",
        "        corpus_embeddings = self.model.encode(n_grams, convert_to_tensor=True)\n",
        "        sentence_embedding = self.model.encode(sentence, convert_to_tensor=True)\n",
        "        # compute similarity scores of the sentence with the corpus\n",
        "        cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
        "        # Sort the results in decreasing order and get the first top_k\n",
        "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
        "        keywords = [(n_grams[idx],cos_scores[idx].item() )for idx in top_results[0:top_k]]\n",
        "        return keywords"
      ],
      "metadata": {
        "id": "zNd4jO6AwrxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST PERFORMING MODELS FOR THIS TASK ::\n",
        "  1. base-nli-stsb-mean-tokens\n",
        "  2. xlm-r-distilroberta-base-paraphase-v1\n",
        "  3. stsb-roberta-large"
      ],
      "metadata": {
        "id": "mzMIhVINz3V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base-nli-stsb-mean-tokens or xlm-r-distilroberta-base-paraphase-v1\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "# model =  SentenceTransformer('stsb-roberta-large')\n",
        "key_extractor = Keyword_Extractor(model)"
      ],
      "metadata": {
        "id": "6MTLn4gZguBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Is Rahul Gandhi's scriptwriter from Peking or Pindi, asks BJP MP Rajyavardhan Rathore\"\n",
        "text = \"Video: Students Stopped At Second Karnataka College Over Hijab\"\n",
        "key_extractor.extract_keywords(text)"
      ],
      "metadata": {
        "id": "wxwPpCCZg2n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOTHER APPROACH "
      ],
      "metadata": {
        "id": "JpJLU1-C0TNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def extract_keywords(doc,model,n_gram_range = (1, 1),stop_words = \"english\",top_n = 5):\n",
        "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
        "    candidates = count.get_feature_names()\n",
        "    doc_embedding = model.encode([doc])\n",
        "    candidate_embeddings = model.encode(candidates)\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "    return keywords\n",
        "keywords = extract_keywords(text,model,n_gram_range = (1, 2),stop_words = \"english\",top_n = 5)\n",
        "keywords"
      ],
      "metadata": {
        "id": "EZ3KhRGHhNzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIND MOST N - SIMILLAR SENTENCES FROM A CORPUS OF SENTENCES"
      ],
      "metadata": {
        "id": "OA7r7AvJ0lPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"I like Python because I can build AI applications\",\n",
        "          \"I like Python because I can do data analytics\",\n",
        "          \"The cat sits on the ground\",\n",
        "         \"The cat walks on the sidewalk\"]\n",
        "# encode corpus to get corpus embeddings\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
        "sentence = \"I like Javascript because I can build web applications\"\n",
        "# encode sentence to get sentence embeddings\n",
        "sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
        "# top_k results to return\n",
        "top_k=2\n",
        "# compute similarity scores of the sentence with the corpus\n",
        "cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
        "# Sort the results in decreasing order and get the first top_k\n",
        "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
        "print(\"Sentence:\", sentence, \"\\n\")\n",
        "print(\"Top\", top_k, \"most similar sentences in corpus:\")\n",
        "for idx in top_results[0:top_k]:\n",
        "    print(corpus[idx], \"(Score: %.4f)\" % (cos_scores[idx]))"
      ],
      "metadata": {
        "id": "CUftw5t6s7rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING KEYWORD EXTRACTOR ----"
      ],
      "metadata": {
        "id": "A8caDDlG2Z5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "scrapping some articles from google news ::--"
      ],
      "metadata": {
        "id": "9SwrkFZY2iaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "D4XBJLauzFh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = ['WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SCIENCE', 'SPORTS', 'HEALTH']\n",
        "def make_topic_url(lang,topic,hours=3):\n",
        "    ceid = f\"when%3A{hours}h&hl={lang}&gl=IN&ceid=IN:{lang}\"\n",
        "    \n",
        "    if topic.upper() in topics:\n",
        "        headlines = f'https://news.google.com/rss/headlines/section/topic/{topic.upper()}?' + ceid\n",
        "        return headlines\n",
        "    else:\n",
        "        return \"invalid topic\"\n",
        "\n",
        "def format_article(article):\n",
        "    data = {}\n",
        "    data['title'] = article.title.text\n",
        "    data['link'] = article.link.text\n",
        "    data['pubDate'] = article.pubDate.text\n",
        "    data['description'] = article.description.text\n",
        "    return data"
      ],
      "metadata": {
        "id": "vkMCY-Rb2r5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang,topic,hours='en','BUSINESS',1\n",
        "topic_url = make_topic_url(lang,topic,hours)\n",
        "response = BeautifulSoup(requests.get(topic_url).content,'xml')\n",
        "articles = [format_article(article) for article in response.find_all(\"item\")]\n",
        "print(len(articles))\n",
        "articles[0]"
      ],
      "metadata": {
        "id": "J6HKY7nr2-kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see the keywods with score greatec than 0.5 are looking good !!!! YAY !! Happy EXtraction"
      ],
      "metadata": {
        "id": "fOQPZTX740XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for article in articles:\n",
        "    text = article['title']\n",
        "    print(text)\n",
        "    keywords = key_extractor.extract_keywords(text,do_lower=True)\n",
        "    print(keywords)\n",
        "    print(\"-----------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "Qv5lHbj23rg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANOTHER APPROACH ||| SIMPLE BASIC |||\n",
        "\n",
        "\n",
        "RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text."
      ],
      "metadata": {
        "id": "a7-8fhWd9EpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rake-nltk"
      ],
      "metadata": {
        "id": "lh_2j_-89WNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rake_nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "t7pNGJLJ9Yhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords_using_rake(string):\n",
        "    rake = rake_nltk.Rake()\n",
        "    rake.extract_keywords_from_text(string)\n",
        "    keywords = rake.get_ranked_phrases()\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "jY0dr_9r4BRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "extract_keywords_using_rake(text)"
      ],
      "metadata": {
        "id": "zzPf9CXo9dAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}