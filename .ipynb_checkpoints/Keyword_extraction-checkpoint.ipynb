{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMbJr9EQgFRI"
   },
   "outputs": [],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UblBd-ZAle3X"
   },
   "source": [
    "SOURCES--\n",
    "  1. https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "  2. https://maartengr.github.io/KeyBERT/\n",
    "  3. https://github.com/ibatra/BERT-Keyword-Extractor\n",
    "\n",
    "\n",
    "for sentence transformers\n",
    "  1. https://www.sbert.net/docs/package_reference/SentenceTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KxX8cwCwuBd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNd4jO6AwrxB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "class Keyword_Extractor:\n",
    "    def __init__(self,model):\n",
    "        if isinstance(model,str):\n",
    "            self.model = SentenceTransformer(model)\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "    def get_ngrams(self,text, n_range=(1,1)):\n",
    "        n_grams = []\n",
    "        a,b = n_range\n",
    "        for i in range(a,b+1):\n",
    "            __ = ngrams(word_tokenize(text), i)\n",
    "            _ = [' '.join(grams) for grams in __]\n",
    "            n_grams = n_grams + _\n",
    "        return n_grams\n",
    "    \n",
    "    def find_simillarity_score(self,sentence1,sentence2):\n",
    "        # encode sentences to get their embeddings\n",
    "        embedding1 = self.model.encode(sentence1, convert_to_tensor=True)\n",
    "        embedding2 = self.model.encode(sentence2, convert_to_tensor=True)\n",
    "        # compute similarity scores of two embeddings\n",
    "        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "        return cosine_scores\n",
    "    \n",
    "    def extract_keywords(self,sentence,ngram_range=(1,2),top_k=5,do_lower=True):\n",
    "        if do_lower:\n",
    "            sentence = sentence.lower()\n",
    "        n_grams = self.get_ngrams(sentence, n_range=ngram_range)\n",
    "        n_grams = list(set([a.strip() for a in n_grams]))\n",
    "        # encode corpus to get corpus embeddings\n",
    "        corpus_embeddings = self.model.encode(n_grams, convert_to_tensor=True)\n",
    "        sentence_embedding = self.model.encode(sentence, convert_to_tensor=True)\n",
    "        # compute similarity scores of the sentence with the corpus\n",
    "        cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "        # Sort the results in decreasing order and get the first top_k\n",
    "        top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "        keywords = [(n_grams[idx],cos_scores[idx].item() )for idx in top_results[0:top_k]]\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzMIhVINz3V_"
   },
   "source": [
    "BEST PERFORMING MODELS FOR THIS TASK ::\n",
    "  1. base-nli-stsb-mean-tokens\n",
    "  2. xlm-r-distilroberta-base-paraphase-v1\n",
    "  3. stsb-roberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MTLn4gZguBA"
   },
   "outputs": [],
   "source": [
    "# base-nli-stsb-mean-tokens or xlm-r-distilroberta-base-paraphase-v1\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "# model =  SentenceTransformer('stsb-roberta-large')\n",
    "key_extractor = Keyword_Extractor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxwPpCCZg2n3"
   },
   "outputs": [],
   "source": [
    "text = \"Is Rahul Gandhi's scriptwriter from Peking or Pindi, asks BJP MP Rajyavardhan Rathore\"\n",
    "text = \"Video: Students Stopped At Second Karnataka College Over Hijab\"\n",
    "key_extractor.extract_keywords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpJLU1-C0TNA"
   },
   "source": [
    "ANOTHER APPROACH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZ3KhRGHhNzq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def extract_keywords(doc,model,n_gram_range = (1, 1),stop_words = \"english\",top_n = 5):\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "    candidates = count.get_feature_names()\n",
    "    doc_embedding = model.encode([doc])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    return keywords\n",
    "keywords = extract_keywords(text,model,n_gram_range = (1, 2),stop_words = \"english\",top_n = 5)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA7r7AvJ0lPy"
   },
   "source": [
    "FIND MOST N - SIMILLAR SENTENCES FROM A CORPUS OF SENTENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUftw5t6s7rz"
   },
   "outputs": [],
   "source": [
    "corpus = [\"I like Python because I can build AI applications\",\n",
    "          \"I like Python because I can do data analytics\",\n",
    "          \"The cat sits on the ground\",\n",
    "         \"The cat walks on the sidewalk\"]\n",
    "# encode corpus to get corpus embeddings\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "sentence = \"I like Javascript because I can build web applications\"\n",
    "# encode sentence to get sentence embeddings\n",
    "sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "# top_k results to return\n",
    "top_k=2\n",
    "# compute similarity scores of the sentence with the corpus\n",
    "cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "# Sort the results in decreasing order and get the first top_k\n",
    "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "print(\"Sentence:\", sentence, \"\\n\")\n",
    "print(\"Top\", top_k, \"most similar sentences in corpus:\")\n",
    "for idx in top_results[0:top_k]:\n",
    "    print(corpus[idx], \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8caDDlG2Z5G"
   },
   "source": [
    "TESTING KEYWORD EXTRACTOR ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SwrkFZY2iaw"
   },
   "source": [
    "scrapping some articles from google news ::--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4XBJLauzFh8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkMCY-Rb2r5r"
   },
   "outputs": [],
   "source": [
    "topics = ['WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SCIENCE', 'SPORTS', 'HEALTH']\n",
    "def make_topic_url(lang,topic,hours=3):\n",
    "    ceid = f\"when%3A{hours}h&hl={lang}&gl=IN&ceid=IN:{lang}\"\n",
    "    \n",
    "    if topic.upper() in topics:\n",
    "        headlines = f'https://news.google.com/rss/headlines/section/topic/{topic.upper()}?' + ceid\n",
    "        return headlines\n",
    "    else:\n",
    "        return \"invalid topic\"\n",
    "\n",
    "def format_article(article):\n",
    "    data = {}\n",
    "    data['title'] = article.title.text\n",
    "    data['link'] = article.link.text\n",
    "    data['pubDate'] = article.pubDate.text\n",
    "    data['description'] = article.description.text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6HKY7nr2-kl"
   },
   "outputs": [],
   "source": [
    "lang,topic,hours='en','BUSINESS',1\n",
    "topic_url = make_topic_url(lang,topic,hours)\n",
    "response = BeautifulSoup(requests.get(topic_url).content,'xml')\n",
    "articles = [format_article(article) for article in response.find_all(\"item\")]\n",
    "print(len(articles))\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOQPZTX740XM"
   },
   "source": [
    "as we can see the keywods with score greatec than 0.5 are looking good !!!! YAY !! Happy EXtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv5lHbj23rg9"
   },
   "outputs": [],
   "source": [
    "for article in articles[:10]:\n",
    "    text = article['title']\n",
    "    print(text)\n",
    "    keywords = key_extractor.extract_keywords(text,do_lower=True)\n",
    "    print(keywords)\n",
    "    print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY0dr_9r4BRt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Keyword_extraction.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
